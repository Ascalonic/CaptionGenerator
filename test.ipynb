{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0607 18:05:26.854416 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W0607 18:05:26.855282 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W0607 18:05:26.855905 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W0607 18:05:26.856508 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W0607 18:05:26.857043 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W0607 18:05:26.857611 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.embedding.embeddings\n",
      "W0607 18:05:26.858392 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.gru.state_spec\n",
      "W0607 18:05:26.859053 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.fc1.kernel\n",
      "W0607 18:05:26.859715 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.fc1.bias\n",
      "W0607 18:05:26.860391 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.fc2.kernel\n",
      "W0607 18:05:26.861116 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.fc2.bias\n",
      "W0607 18:05:26.861831 140090084394752 util.py:144] Unresolved object in checkpoint: (root).encoder.fc.kernel\n",
      "W0607 18:05:26.866993 140090084394752 util.py:144] Unresolved object in checkpoint: (root).encoder.fc.bias\n",
      "W0607 18:05:26.867791 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.gru.cell.kernel\n",
      "W0607 18:05:26.868448 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.gru.cell.recurrent_kernel\n",
      "W0607 18:05:26.869113 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.gru.cell.bias\n",
      "W0607 18:05:26.869781 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.attention.W1.kernel\n",
      "W0607 18:05:26.870445 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.attention.W1.bias\n",
      "W0607 18:05:26.871068 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.attention.W2.kernel\n",
      "W0607 18:05:26.871670 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.attention.W2.bias\n",
      "W0607 18:05:26.872336 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.attention.V.kernel\n",
      "W0607 18:05:26.872909 140090084394752 util.py:144] Unresolved object in checkpoint: (root).decoder.attention.V.bias\n",
      "W0607 18:05:26.873502 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.embedding.embeddings\n",
      "W0607 18:05:26.874143 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc1.kernel\n",
      "W0607 18:05:26.874718 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc1.bias\n",
      "W0607 18:05:26.875326 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc2.kernel\n",
      "W0607 18:05:26.878999 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc2.bias\n",
      "W0607 18:05:26.879652 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.fc.kernel\n",
      "W0607 18:05:26.880243 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.fc.bias\n",
      "W0607 18:05:26.880827 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.kernel\n",
      "W0607 18:05:26.881546 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.recurrent_kernel\n",
      "W0607 18:05:26.882110 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.bias\n",
      "W0607 18:05:26.882788 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.kernel\n",
      "W0607 18:05:26.883381 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.bias\n",
      "W0607 18:05:26.883908 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.kernel\n",
      "W0607 18:05:26.884603 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.bias\n",
      "W0607 18:05:26.891059 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.kernel\n",
      "W0607 18:05:26.891734 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.bias\n",
      "W0607 18:05:26.892282 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.embedding.embeddings\n",
      "W0607 18:05:26.892960 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc1.kernel\n",
      "W0607 18:05:26.893581 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc1.bias\n",
      "W0607 18:05:26.894407 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc2.kernel\n",
      "W0607 18:05:26.896649 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc2.bias\n",
      "W0607 18:05:26.897177 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.fc.kernel\n",
      "W0607 18:05:26.897828 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.fc.bias\n",
      "W0607 18:05:26.906597 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.kernel\n",
      "W0607 18:05:26.907122 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.recurrent_kernel\n",
      "W0607 18:05:26.907818 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.bias\n",
      "W0607 18:05:26.908524 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.kernel\n",
      "W0607 18:05:26.909266 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.bias\n",
      "W0607 18:05:26.910008 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.kernel\n",
      "W0607 18:05:26.910730 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.bias\n",
      "W0607 18:05:26.911420 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.kernel\n",
      "W0607 18:05:26.912115 140090084394752 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.bias\n",
      "W0607 18:05:26.912824 140090084394752 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-66ef25a2fe33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    220\u001b[0m                                      origin=image_url)\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Prediction Caption:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0mplot_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-66ef25a2fe33>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_features_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_length' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# To geenrate plots of attention in order to see which parts of an image\n",
    "# the model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "annotation_file = 'annotations/captions_train2014.json'\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap\n",
    "\n",
    "\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "    \n",
    "# Download image files\n",
    "image_folder = '/train2014/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(image_zip) + image_folder\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + image_folder\n",
    "\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Store captions and image names in vectors\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "# Set a random state\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "\n",
    "\n",
    "top_k = 5000\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "image_url = 'https://cdn.kqed.org/wp-content/uploads/sites/10/2020/01/marketstreet200122a.jpg'\n",
    "image_extension = image_url[-4:]\n",
    "image_path = tf.keras.utils.get_file('image'+image_extension,\n",
    "                                     origin=image_url)\n",
    "\n",
    "result, attention_plot = evaluate(image_path)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image_path, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
